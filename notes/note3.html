<!DOCTYPE html>
<!-- saved from url=(0087)file:///C:/Users/ISE/Desktop/shindai%20programming%20language%20I%20homework%203-2.html -->
<html lang="ja">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="../notestyle.css">
    <link rel="stylesheet" href="../button.css">
    <title>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート3</title>
     <!--
    <script type="text/javascript" src="./config.js" defer></script>
    TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    N: "{\\mathrm{N}}",
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\displaystyle{\mathrm{max}}}"
                },
                extensions: [
                    "cancel.js",           // 抹消線を有効にする。
                    "color.js"             // 文字の色付けを有効にする。
                ]
            },
    -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    Lim: ["{\\displaystyle{\\lim_{#1 \\to \\infty}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\mathrm{max}}",
                    N: "{\\mathbb{N}}",
                    R: "{\\mathbb{R}}",
                    min: "{\\mathrm{min}}",
                    tx: "{\\tilde{x}}",
                },
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\"] ],
                processEscapes: true
            }, 
            "HTML-CSS": {
                matchFontHeight: false,
                undefinedFamily: "Meiryo, STIXGeneral, 'Arial Unicode MS', serif",
                mtextInheritFont: true,
                scale: 100
            },
            displayAlign: "left",
            displayIndent: "4em"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


<body>


    <div class="document" id="top">
        <h1><span>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート3</span></h1>
        <p style="text-align:right;">電気通信大学大学院 情報理工学研究科 情報学専攻 1年 鈴木一生(2430073)</p>
        <h2>リンク集</h2>
        <div class="th" id="link">
            <span class="th-t" style="width:10em;">
                <h5>ページ構成(リンク)</h5>
            </span>
            <ul>
                <li><a href="#first">第3章 クラスタリング</a></li>
                <ul>
                    <li><a href="#1">1</a>，<a href="#2">2</a>，<a href="#3">3</a>，</li>
                    <li><a href="#3.3">例題3.3</a>，<a href="#3.4">例題3.4</a>，</li>
                    <li><a href="#4">4</a>，<a href="#5">5</a></li>
                </ul>
            </ul>
        </div>


        <h2 id="first">第3章 クラスタリング</h2>
        <h3>章末問題(p. 97)</h3>
        <div class="def " id="1">
            <span class="def-t ">
                <h4>【 1 】</h4>
            </span>
            <p>例題3.1と同じ1次元のデータ$D$が与えられたとする: 
                $$D = \{0, 1, 3, 5.5\}$$
            </p>
            <p>事例間の類似度は，差の逆数で測ることにする。このデータに対し，重心法による凝集型クラスタリングを実行せよ。もし，二つのクラスタ対について類似度が等しい場合は，融合後のクラスタが小さいものを優先して融合することにする。</p>
        </div>

        <div class="th">
            <span class="th-t">
                <h5>方針</h5>
            </span>
            <p>重心法は，与えられた2つのクラスタに対し，それらの重心間の類似度をこれらのクラスタ間の類似度とするから，数式にして，
                $$sim(c_i, c_j) = sim \left(\frac{1}{|c_i|} \sum_{\boldsymbol{x}\in c_i} \boldsymbol{x}, \; \frac{1}{|c_j|}\sum_{\boldsymbol{x} \in c_j} \boldsymbol{x} \right)$$
                と表される。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>昇順に並べた事例の間に$|$を置くことでクラスタを表現することにする。</p>
            <p>$|$から$|$までが1つのクラスタである。</p>
            <p>このとき，クラスタリングは上から順に次のように進む。</p>
            <p>
                $$|\; 0 \;|\; 1 \;|\; 3 \;|\; 5.5 \;|$$
                $$|\; 0 \; 1 \;|\; 3 \;|\; 5.5 \;|\qquad \qquad \left(\because 類似度が左から，1,\frac{1}{2}, \frac{1}{2.5} \right)$$
                $$|\; 0 \; 1 \;|\; 3 \; 5.5 \;|\quad \left(\because 類似度が左から，\frac{1}{2.5}, \frac{1}{2.5}   だから，融合後のクラスタが小さい3,5.5を融合\right)$$
                $$|\; 0 \; 1 \; 3 \; 5.5 \;|$$
            </p>

        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="2">
            <span class="def-t ">
                <h4>【 2 】</h4>
            </span>
           <p>例題3.2と同じ1次元のデータ$D$が与えられたとする:
                $$D = \{0, 1, 3, 5.5\}$$
           </p>
           <p>$k = 2$として$k-$平均法を用いてクラスタリングを実行せよ。ただし，初期状態において，クラスタ$c_1$に対応する代表値(一般には代表ベクトル)は$m_1 = -1$，クラスタ$c_2$に対応する代表値は$m_2 = 7.5$とする。</p>
        </div>

        <div class="th">
            <span class="th-t">
                <h5>方針</h5>
            </span>
            <p>k-means法においては，次のステップを経てクラスタリングを行う。</p>
            <p>
                
            </p>
            <ol>
                <li>各点に対してランダムにクラスタを割り振る。</li>
                <li>各クラスタに割り当てられた点について重心を計算する。</li>
                <li>各点について上記で計算された重心からの距離を計算し、距離が一番近いクラスタに割り当て直す。</li>
                <li>2.と3.の工程を、割り当てられるクラスタが変化しなくなるまで行う。</li>
            </ol>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>方針に示した通りの手順で，k-means法を用いたクラスタリングを試みる。</p>
            <p>事例集合の分割: $c_1 = \{0, 1, 3\},\; c_2 = \{5.5\}$</p>
            <p>代表値の再計算: $m_1 = \cfrac{0 + 1 + 3}{3} = 1.33\cdots ,\; m_2 = 5.5$</p>
            <p>事例集合の分割: $c_1 = \{0, 1, 3\},\; c_2 = \{5.5\}$</p>
            <p>以降クラスタに変化がないため，これで終了。</p>
        </div>

        <div class="def " id="3.3">
            <span class="def-t ">
                <h4>例題3.3</h4>
            </span>
            <p>ポアソン分布の場合について考えてみよう。</p>
            <p>各クラスタ$c$について，正規分布の代わりにポアソン分布$P(x|c)$を仮定する:
                $$P(x|c) = \frac{\mu_c^x}{x!} e^{-\mu_c}$$
            </p>
            <p>データ$D = \{x^{(1)}, x^{(2)}, \cdots , x^{(|D|)}\}$が与えられたとして，EMアルゴリズムにおけるEステップとMステップを求めよ。</p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="th">
            <span class="th-t">
                <h5>方針</h5>
            </span>
            <p>$Q$関数は次のように表される。</p>
            <p>$$Q(\theta; \theta') = \sum_{\boldsymbol{x}^{(i)} \in D} \sum_c P(c|\boldsymbol{x}^{(i)}; \theta') \log P(c, \boldsymbol{x}^{(i)}; \theta)$$</p>
            <p>また，EMアルゴリズムは収束するまで，次のステップを繰り返す。</p>
            <br>
            <p>Eステップ: 任意の$\boldsymbol{x}^{(i)} \in D$，任意の$c$について，$P(c|\boldsymbol{x}^{(i)}; \theta')$を計算</p>
            <p>Mステップ: $\theta^{max} = \mathrm{arg}\underset{\theta}{\max} Q(\theta; \theta')$</p>
            <p>$\theta' = \theta^{max}$</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <div class="flex">
                <div class="text" style="width: 60%;">
                    <p>まず$Q$関数は，
                        $$Q(\mu, p; \mu', p') = \sum_{x^{(i)}\in D} \sum_c P(c|x^{(i)}; \mu_c', p'_c) \log p_c \frac{\mu_c^{x(i)}}{x^{(i)}!} e^{-\mu_c}$$
                        となる。
                    </p>
                    <p>Eステップは，
                        $$\begin{align*} P(c|x^{(i)}; \mu'_c, p'_c) &= \cfrac{p'_c \cfrac{\mu{'x^{(i)}}_c}{x^{(i)}!} e^{-\mu'_c}}{\dis{\sum_c} p'_c\cfrac{\mu^{'x^{(i)}}_c}{x!} e^{-\mu'_c}} \\
                            &= \cfrac{p'_c \mu^{'x}_c e^{-\mu'_c} }{\dis{\sum_c} p'_c \mu^{'x}_c e^{-\mu'_c}}
                        \end{align*}$$
                    </p>
                    <p>Mステップにおいては，$Q$関数を最大化する$\mu_c$を求めればよいから，$\mu_c$で偏微分して，
                        $$\begin{align*}\frac{\partial Q(\mu_c, p_c; \mu'_c, p'_c)}{\partial \mu_c} &= \sum_{x^{(i)}\in D} P(c|x^{(i)}; \mu'_c, p'_c) \left(\frac{x^{(i)}}{\mu_c} - 1\right) \\
                        &= \frac{1}{\mu_c} \sum_{x^{(i)}\in D} P(c|x^{(i)}; \mu'_c, p'_c)x^{(i)} - \sum_{x^{(i)} \in D} P(c|x^{(i)}; \mu'_c, p'_c) \end{align*}$$
                    </p>
                    <p>よって，この式が0と等しくなる$\mu_c$を求めると，
                        $$\mu_c = \frac{\dis{\sum_{x^{(i)}\in D}} P(c|x^{(i)})x^{(i)}}{\dis{\sum_{x^{(i)}\in D}} P(c|x^{(i)})}$$
                    </p>
                </div>
                <div class="caption" style="width: 40%;">
                    <p>$Q$関数における対数尤度の部分は，任意のデータの事例$x^{(i)}$において，クラスタ$c$に属している確率とポアソン分布の積の対数となる。</p>
                    <p>Eステップの式変形解説</p>
                    <p>$$
                        \begin{align*} P(c|\boldsymbol{x}^{(i)}) &= \frac{P(\boldsymbol{x}^{(i)}, c)}{P(\boldsymbol{x}^{(i)})} \\
                        &= \frac{P(c, \boldsymbol{x}^{(i)})}{\dis{\sum_c}P(c, \boldsymbol{x}^{(i)})} \\
                        &= \frac{P(c)P(\boldsymbol{x}^{(i)}|c)}{\dis{\sum_c} P(c)P(\boldsymbol{x}^{(i)}|c)}\end{align*}$$</p>
                    <p>Mステップの偏微分解説</p>
                    <p>$\log p_c \cfrac{\mu_c^{x^{(i)}}}{x^{(i)}!} e^{-\mu_c} = \log \cfrac{p_c}{x^{(i)}!} \mu_c^{x^{(i)}} - \mu_c$</p>
                </div>
            </div>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="3.4">
            <span class="def-t ">
                <h4>例題3.4</h4>
            </span>
            <p>確率変数$\; X \;$と$\; Y \;$の同時確率が，
                $$\begin{align*} P(X = x, Y = y) &= \sum_c P(x, y, c) \\
                &= \sum_c P(x|c) P(y|c) P(c)\end{align*}$$
                と表されるとしよう。
            </p>
            <p>3種類のパラメータを$\; q_{x,c} = P(x|c), \; r_{y, c} = P(y|c),\; p_c = P(c)\;$とおく。</p>
            <p>$\; c \;$は隠れ変数$\;C\;$の値であるとして，EMアルゴリズムにおけるEステップ及びMステップの更新式を求めよ。</p>
            <p>与えられているデータは，$\; D = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots , (x^{(|D|)}, y^{(|D|)}) \}\;$であるとする。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$Q$関数は次のようになる。</p>
            <p>$$Q(\theta; \theta') = \sum_{x, y} n_{x, y} \sum_c P(c|x, y; \theta') \log q_{x, c} \; r_{y, c} \; p_c$$</p>
            <p>Eステップは，
                $$P(c|x, y; \theta') = \frac{q'_{x, c} \; r'_{y, c} \; p'_c}{\dis{\sum_c} q'_{x, c} \; r'_{y, c} \; p'_c}$$
                となる。
            </p>
            <p>Mステップで最大化するにあたり，ラグランジュ関数$L(\theta, \lambda_1, \lambda_2, \lambda_3)$を
                $$\begin{align*} L(\theta, \lambda_1, \lambda_2, \lambda_3) &= Q(\theta; \theta') + \sum_c \lambda_1 \left(\sum_x q_{x, c} - 1 \right) + \sum_c \lambda_2 \left(\sum_y q_{y, c} - 1 \right) + \lambda_3 \left(\sum_c p_c - 1 \right)  
                \end{align*}$$
                とおく。
            </p>
            <p>よって，ラグランジュ関数をそれぞれの変数で偏微分した結果が0と等しいものとして，等式を作り，それらの連立方程式を解くことで，最大値が求められるから，
                $$
                \left\{ \,
                    \begin{aligned}
                    
                & \frac{\partial L(\theta, \lambda_1, \lambda_2, \lambda_3)}{\partial q_{x, c}} = \sum_y n_{x, y} P(c|x, y; \theta') \frac{1}{q_{x, c}} + \lambda_1 = 0 \\
                & \frac{\partial L(\theta, \lambda_1, \lambda_2, \lambda_3)}{\partial \lambda_1} = \sum_x q_{x, c} - 1 = 0
                
                \end{aligned}
                \right.
                $$
                より，
                $$q_{x, c} = \frac{\dis{\sum_y} n_{x,y} P(c|x, y; \theta') }{\dis{\sum_x \sum_y} n_{x, y} P(c|x, y; \theta')}$$
                となる。
            </p>
            <p>同様にして，
                $$r_{y, c} = \frac{\dis{\sum_x} n_{x,y} P(c|x, y; \theta') }{\dis{\sum_y \sum_x} n_{x, y} P(c|x, y; \theta')}$$
                $$p_c = \frac{\dis{\sum_{x, y}} n_{x,y} P(c|x, y; \theta') }{\dis{\sum_c \sum_{x,y}} n_{x, y} P(c|x, y; \theta')}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="3">
            <span class="def-t ">
                <h4>【 3 】</h4>
            </span>
            <p>3.5節で登場した混合正規分布について，この分布をEMアルゴリズムを用いて推定するときのMステップにおける更新式を求めよ(式(3.6))。$Q$関数は
                $$Q(\theta; \theta') = \sum_{\bm{x}^(i) \in D} \sum_c \overline{P} (c| \bm{x}^{(i)}; \theta') \log P(c) \frac{1}{\sqrt{(2\pi \sigma^2)^d}} \exp \left(- \frac{|\bm{x}^(i) - \bm{m}_c|^2}{2 \sigma^2} \right)$$
                と表されているので，これを最大化する$\bm{m}_c$を求めればよい。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$Q$関数を$m_c$で偏微分すると，
                $$\begin{align*} \nabla_{\boldsymbol{m_c}} Q(\theta; \theta') &=  \sum_{\bm{x}^{(i)} \in D} \overline{P} (c| \bm{x}^{(i)}; \theta') \nabla_{\boldsymbol{m_c}} \left\{- \frac{|\bm{x}^{(i)} - \bm{m}_c|^2}{2 \sigma^2} \right\} \\
                &=  \sum_{\bm{x}^{(i)} \in D} \overline{P} (c| \bm{x}^{(i)}; \theta') \frac{\bm{x}^{(i)} - \bm{m}_c}{ \sigma^2}
                \end{align*}$$
            </p>
            <p>したがって，この式が$\boldsymbol{0}$と等しいものとして，解くと，
                $$\boldsymbol{m_c} = \frac{\dis{\sum_{\boldsymbol{x}^{(i)}\in D} \overline{P} (c|\boldsymbol{x}^{(i)}; \theta') \boldsymbol{x}^{(i)}}}{ \dis{\sum_{\boldsymbol{x}^{(i)}\in D}} \overline{P}(c|\boldsymbol{x}^{(i)}; \theta')}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="4">
            <span class="def-t ">
                <h4>【 4 】</h4>
            </span>
            <p>文章$d$の生成確率が，$\dis{\sum_c P(c) P(d | c)}$という混合分布で表されているとする。各クラスタ$c$について多項分布$P(d|c)$を仮定する。ただし，この$d$は長さ$n$の文書であり，$m$個の単語集合から一つの単語を選ぶ試行を$n$回繰り返すことで生成されるとする。$n$個の単語トークンのうち$w$であるものの数を$n_{w, d}$でクラスタが$c$であるときに単語$w$を選ぶ確率を$\bm{q}_{w, c}$で表すと，
                $$P(d|c) = \frac{n!}{\dis{\prod_w} n_{w, d}!} \prod_w q_{w, c}^{n_{w, d}}$$
                となる。クラスタを選ぶ確率$P(c)$を$p_c$で表すとして，EMアルゴリズムにおけるEステップとMステップを求めよ。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$Q$関数を次のように定義する。</p>
            <p>$$Q(\theta; \theta') = \sum_{d \in D} \sum_c P(c|d; \theta') \log p_c \frac{n!}{\dis{\prod_w} n_{w, d}!} \prod_w q_{w, c}^{n_{w, d}}$$</p>
            <p>Eステップは，
                $$\begin{align*} P(c|d; \theta') &= \cfrac{p_c \cfrac{n!}{\dis{\prod_w} n_{w, d}!} \prod_w q_{w, c}^{n_{w, d}}}{\dis{\sum_c}p_c \frac{n!}{\dis{\prod_w} n_{w, d}!} \dis{\prod_w} q_{w, c}^{n_{w, d}} } \\
                &= \frac{p_c \dis{\prod_w} q_{w, c}^{n_{w, d}}}{\dis{\sum_c} p_c \dis{\prod_w} q_{w, c}^{n_{w, d}}}
                \end{align*}$$
                となる。
            </p>
            <p>Mステップで最大化するにあたり，ラグランジュ関数$L(\theta, \lambda_1, \lambda_2)$を次のように定義する。</p>
            <p>$$L(\theta, \lambda_1, \lambda_2) = Q(\theta; \theta') + \lambda_1 \left(\sum_c p_c - 1 \right) + \sum_c \lambda_2 \left(\sum_w q_{w, c} - 1 \right)$$</p>
            <p>ここで，ラグランジュ関数を各変数において偏微分して，0と等しいものとして，連立方程式を解くと，
                $$\left\{ \,
                \begin{aligned}
                &\frac{\partial L(\theta, \lambda_1, \lambda_2)}{\partial p_c} = \sum_{d \in D} P(c|d; \theta') \frac{1}{p_c} + \lambda_1 = 0 \\
                &\frac{\partial L(\theta, \lambda_1, \lambda_2)}{\partial q_{w, c}} = \sum_{d \in D} P(c|d; \theta') \frac{n_{w, d}}{q_{w, c}} + \lambda_2 = 0 \\
                &\frac{\partial L(\theta, \lambda_1, \lambda_2)}{\partial \lambda_1} = \sum_c p_c - 1 = 0 \\
                &\frac{\partial L(\theta, \lambda_1, \lambda_2)}{\partial \lambda_2} = \sum_w q_{w, c} - 1 = 0
                        
                \end{aligned}
                \right.$$
                より，
                $$p_c = \frac{\dis{\sum_{d \in D}} P(c|d; \theta')}{\dis{\sum_c} \dis{\sum_{d \in D}} P(c|d; \theta')}$$
                $$p_c = \frac{\dis{\sum_{d \in D}} P(c|d; \theta') n_{w, d}}{\dis{\sum_w} \dis{\sum_{d \in D}} P(c|d; \theta') n_{w, d}}$$
                となる。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="5">
            <span class="def-t ">
                <h4>【 5 】</h4>
            </span>
            <p>例題3.4と同じデータが与えられたとする:
                $$D = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}, \cdots , (x^{(|D|)}, y^{(|D|)})) \}$$
                ここでは，確率変数$X$と$Y$の同時確率が
                $$\begin{align*} 
                    P(X = x, Y = y) &= \sum_{c, d} P(x, y, c, d) \\
                    &= \sum_{c, d} P(x|c) P(y | d) P(c, d)
                \end{align*}$$
                と表されるとしよう(例題3.4のモデルがアスペクトモデルとよばれるのに対し，このモデルは<em>プロダクトモデル</em>(product model)とよばれる)。このモデルは二つの隠れ変数を持つ。3種類のパラメータを$q_{x, c} = P(x | c), r_{y, d} = P(y | d), p_{c, d} = P(c, d)$とおく。EMアルゴリズムにおけるEステップおよびMステップの更新式を求めよ。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$Q$関数は次のようになる。</p>
            <p>$$Q(\theta; \theta') = \sum_{x, y} n_{x, y} \sum_{c, d} P(c|x, y; \theta') \log q_{x, c} \; r_{y, c} \; p_{c, d}$$</p>
            <p>Eステップは，
                $$P(c, d|x, y; \theta') = \frac{q'_{x, c} \; r'_{y, c} \; p'_{c, d}}{\dis{\sum_c} q'_{x, c} \; r'_{y, c} \; p'_{c, d}}$$
                となる。
            </p>
            <p>Mステップで最大化するにあたり，ラグランジュ関数$L(\theta, \lambda_1, \lambda_2, \lambda_3)$を
                $$\begin{align*} L(\theta, \lambda_1, \lambda_2, \lambda_3) &= Q(\theta; \theta') + \sum_c \lambda_1 \left(\sum_x q_{x, c} - 1 \right) + \sum_c \lambda_2 \left(\sum_y q_{y, c} - 1 \right) + \lambda_3 \left(\sum_c p_{c, d} - 1 \right)  
                \end{align*}$$
                とおく。
            </p>
            <p>よって，ラグランジュ関数をそれぞれの変数で偏微分した結果が0と等しいものとして，等式を作り，それらの連立方程式を解くことで，最大値が求められるから，
                $$
                \left\{ \,
                    \begin{aligned}
                    
                & \frac{\partial L(\theta, \lambda_1, \lambda_2, \lambda_3)}{\partial q_{x, c}} = \sum_y n_{x, y} P(c, d|x, y; \theta') \frac{1}{q_{x, c}} + \lambda_1 = 0 \\
                & \frac{\partial L(\theta, \lambda_1, \lambda_2, \lambda_3)}{\partial \lambda_1} = \sum_x q_{x, c} - 1 = 0
                
                \end{aligned}
                \right.
                $$
                より，
                $$q_{x, c} = \frac{\dis{\sum_y} n_{x,y} P(c, d|x, y; \theta') }{\dis{\sum_x \sum_y} n_{x, y} P(c, d|x, y; \theta')}$$
                となる。
            </p>
            <p>同様にして，
                $$r_{y, c} = \frac{\dis{\sum_x} n_{x,y} P(c, d|x, y; \theta') }{\dis{\sum_y \sum_x} n_{x, y} P(c, d|x, y; \theta')}$$
                $$p_{c, d} = \frac{\dis{\sum_{x, y}} n_{x,y} P(c, d|x, y; \theta') }{\dis{\sum_c \sum_{x,y}} n_{x, y} P(c, d|x, y; \theta')}$$
            </p>
        </div>


        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>
        <div class="flex">
            <p style="width:33%;"><a href="./note2.html " class="btn-square-slant ">←前のページに移る</a></p>
            <p style="width:33%;"><a href="../index.html " class="btn-square-slant ">トップページに戻る</a></p>
            <p style="width:33%;"><a href="./note4.html " class="btn-square-slant ">次のページに移る→</a></p>
        </div>

        <br>

</body>

</html>