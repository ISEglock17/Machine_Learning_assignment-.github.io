<!DOCTYPE html>
<!-- saved from url=(0087)file:///C:/Users/ISE/Desktop/shindai%20programming%20language%20I%20homework%203-2.html -->
<html lang="ja">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="../notestyle.css">
    <link rel="stylesheet" href="../button.css">
    <title>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート4</title>
     <!--
    <script type="text/javascript" src="./config.js" defer></script>
    TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    N: "{\\mathrm{N}}",
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\displaystyle{\mathrm{max}}}"
                },
                extensions: [
                    "cancel.js",           // 抹消線を有効にする。
                    "color.js"             // 文字の色付けを有効にする。
                ]
            },
    -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    Lim: ["{\\displaystyle{\\lim_{#1 \\to \\infty}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\mathrm{max}}",
                    N: "{\\mathbb{N}}",
                    R: "{\\mathbb{R}}",
                    min: "{\\mathrm{min}}",
                    tx: "{\\tilde{x}}",
                    bx:["{\\boldsymbol{x}^{(#1)}}", 1],
                },
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\"] ],
                processEscapes: true
            }, 
            "HTML-CSS": {
                matchFontHeight: false,
                undefinedFamily: "Meiryo, STIXGeneral, 'Arial Unicode MS', serif",
                mtextInheritFont: true,
                scale: 100
            },
            displayAlign: "left",
            displayIndent: "4em"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


<body>


    <div class="document" id="top">
        <h1><span>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート4</span></h1>
        <p style="text-align:right;">電気通信大学大学院 情報理工学研究科 情報学専攻1年 鈴木一生(2430073)</p>
        <h2>リンク集</h2>
        <div class="th" id="link">
            <span class="th-t" style="width:10em;">
                <h5>ページ構成(リンク)</h5>
            </span>
            <ul>
                <li><a href="#forth">第4章 分類</a></li>
                <ul>
                    <li><a href="#4--2">4.2.1 [2] 多変数ベルヌーイモデルのパラメータの最尤推定</a></li>
                    <li><a href="#4--3">4.2.1 [3] 多変数ベルヌーイモデルのパラメータのMAP推定</a></li>
                </ul>
                <li><a href="#shomatsu">章末問題</a></li>
                <ul>
                    <li><a href="#1">【1】</a></li>
                </ul>
                <li><a href="">教科書</a></li>
                <ul>
                    <li><a href="#4-2--2">4.2.2 [2] 多項モデルのパラメータの最尤推定</a></li>
                    <li><a href="#4-2--3">4.2.2 [3] 多項モデルのパラメータのMAP推定</a></li>
                </ul>
                <li><a href="#shomatsu2">章末問題</a></li>
                <ul>
                    <li><a href="#2">【2】</a></li>
                </ul>
                <ul>
                    <li><a href="#3">【3】</a></li>
                </ul>
                <ul>
                    <li><a href="#4">【4】</a></li>
                </ul>
                <li><a href="#rei">教科書</a></li>
                <ul>
                    <li><a href="#rei">例題4.11</a></li>
                </ul>
                <li><a href="">章末問題</a></li>
                <ul>
                    <li><a href="#5">【5】</a></li>
                </ul>
                <ul>
                    <li><a href="#6">【6】</a></li>
                </ul>
                <li><a href="">教科書</a></li>
                <ul>
                    <li><a href="#4-5-2">4.5.2 対数線形モデルの更新式の導出</a></li>
                </ul>
            </ul>
        </div>


        <h2 id="forth">第4章 分類</h2>
        <h3>教科書</h3>
        <div class="def " id="4--2">
            <span class="def-t ">
                <h4>4.2.1 [2] 多変数ベルヌーイモデルのパラメータの最尤推定(p. 103)</h4>
            </span>
            <p>最尤推定を用い，与えられたデータからパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>教科書103ページの式(4.6)に示されている，ナイーブベイズ分類器における多変数ベルヌーイモデルのパラメータにおける最尤推定であるから，
                $$\begin{align*} \log P(D) &= \sum_{(d, c) \in D} \log P(d, c) \\ &= \sum_{(d, c) \in D} \log \left(p_c \prod_{w\in V} \left(p_{w, c}^{\delta_{w, d}} (1 - p_{w, c})^{1 - \delta_{w, d}} \right) \right) \\ &=  \sum_{(d, c) \in D} \left(\log  p_c + \sum_{w\in V} \left( \delta_{w, d}\log p_{w, c} + (1 - \delta_{w, d}) \log (1 - p_{w, c}) \right) \right)\end{align*}$$
            </p>
            <p>ここで，
                $$N_{w, c}: \text{クラス$c$であり，かつ$w$を含むような訓練文書の数}$$
                $$N_{c}: \text{クラス$c$であるような訓練文書の数}$$
                と表すことにすると，$d$に関する総和を考えて，次のようになる。
            </p>
            <p>$$\log P(D) = \sum_{c} N_c \log  p_c + \sum_{c} \sum_{w\in V}  N_{w, c}\log p_{w, c} + \sum_{c} (N_c - N_{w, c}) \log (1 - p_{w, c})  $$</p>
            <p>したがって，この$\log P(D)$を制約条件$\dis{\sum_c p_c = 1}$のもとで，最大化すればよいので，ラグランジュ関数$L(p_{w, c}, \; p_c,\; \lambda)$は，
                $$L(p_{w, c}, \; p_c,\; \lambda) = \sum_{c} N_c \log  p_c + \sum_{c} \sum_{w\in V}  N_{w, c}\log p_{w, c} \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  + \sum_{c} (N_c - N_{w, c}) \log (1 - p_{w, c}) + \lambda \left(\sum_c p_c - 1 \right)$$
            </p>
            <p>よって，このラグランジュ関数$L(p_{w, c}, \; p_c,\; \lambda)$を各変数で偏微分した式それぞれが0に等しくなるような変数の値が最大値となるから，
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial p_{w, c}} = \frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0 \cdots \cdots ①$$
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial p_{c}} = \frac{N_c}{p_c} + \lambda = 0 \cdots \cdots ➁$$
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial \lambda} = \sum_c p_c - 1 = 0 \cdots \cdots ③$$</p>
            <p>①より，$\frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0 $に対して，両辺$p_{w, c} (1 - p_{w, c})$をかけて，
                $$N_{w, c} - N_{w, c} p_{w, c} = N_c p_{w, c} - N_{w, c} p_{w, c} $$
                $$\therefore p_{w, c} = \frac{N_{w, c}}{N_c}$$
            </p>
            <p>➁より，$p_c = - \cfrac{N_c}{\lambda}$となるから，これを③に代入して，
                $$\sum_c - \cfrac{N_c}{\lambda} = 1 \qquad \therefore \lambda = - \sum_{c} N_c$$
            </p>
            <p>よって，$p_c = \cfrac{N_c}{\dis{\sum_c} N_c}$となる。</p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="4--3">
            <span class="def-t ">
                <h4>4.2.1 [3] 多変数ベルヌーイモデルのパラメータのMAP推定(p. 107)</h4>
            </span>
            <p>MAP推定を用い，与えられたデータからパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>MAP推定を行うにあたって，ディリクレ分布を事前分布に用いることにして，目的関数となるものを計算すると，
                $$\begin{align*} 
                    \log P(\theta) + \log P(D) &= \log \left(\prod _c p_c^{\alpha - 1} \right) \times \left(\prod_{w, c} \left(p_{w, c}^{\alpha - 1} (1 - p_{w, c})^{\alpha - 1} \right)\right) + \sum_{(d, c) \in D} \log P(d|c) + \mathrm{const.} \\
                    &= (\alpha - 1)\sum_c \log p_c + (\alpha - 1)\sum_{w, c}\left(\log p_{w, c} + \log(1 - p_{w, c}) \right) \\ &\qquad \qquad \qquad \qquad  + \sum_{(d, c) \in D} \log \left(p_c \prod_{w \in V} \left(p_{w, c}^{\delta_{w, c}}(1 - p_{w, c})^{1 - \delta_{w, d}} \right) \right) + \mathrm{const.}
                \end{align*}$$
                となる。
            </p>
            <p>したがって，これを最大化するから，ラグランジュの未定乗数法を用いる。</p>
            <p>ラグランジュ関数$L(\theta, \lambda)$は，
                $$L(\theta, \lambda) = \log P(\theta) + \log P(D) + \lambda \left(\sum_c p_c - 1 \right)$$
                となる。
            </p>
            <p>よって，このラグランジュ関数を各変数で偏微分して，それぞれの式が0と等しいものとして，連立方程式を解くことで最大化できるから，ラグランジュ関数を各変数で偏微分して，
                $$\frac{\partial L(\theta, \lambda)}{\partial p_{w, c}} = \frac{\alpha - 1}{p_{w, c}} + \frac{\alpha - 1}{1 - p_{w, c}} + \frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0$$
                $$\frac{\partial L(\theta, \lambda)}{\partial p_c} = \frac{\alpha - 1}{p_c} + \frac{N_c}{p_c} + \lambda = 0$$
                $$\frac{\partial L(\theta, \lambda)}{\partial \lambda} = \sum_c p_c - 1 = 0$$
                となる。
            </p>
            <p>したがって，これらを解くと，
                $$p_{w, c} = \frac{N_{w, c} + \alpha - 1}{N_c + 2(\alpha - 1)}$$
                $$p_c = \frac{N_c + \alpha - 1}{\dis{\sum_c} N_c + |C| (\alpha - 1)}\qquad (|C|\text{はクラス数を示す})$$
                となる。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>章末問題(p. 145)</h3>
        <div class="def " id="1">
            <span class="def-t ">
                <h4>【 1 】</h4>
            </span>
            <p>文書の長さがクラスに依存するような，多項モデルのナイーブベイズ分類器を作ってみる。文書の長さは(クラスに依存する)ポアソン分布に従うとして，$P(d|c)$を式で表せ。また，最尤推定を用いてパラメータを算出するための式を求めよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>ポアソン分布に従うことから，$P(d|c)$は，
                $$\begin{align*}
                P(d|c) &= \frac{\mu_c^{\dis{\sum_w} n_{w, d}}}{\left(\dis{\sum_w} n_{w, d} \right)!} e^{- \mu_c} \frac{\left(\dis{\sum_w}n_{w, d} \right)!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}} \\
                &= \mu_c^{\dis{\sum_w} n_{w, d}} e^{-\mu_c} \frac{1}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w\in V} q_{w, c}^{n_{w, d}}
                \end{align*}$$
                となる。
            </p>
            <p>ここで，対数尤度は，
                $$\log P(D) = \sum_{(d, c) \in D} \left(\sum_w n_{w, d} \right) \log \mu_c - \sum_{(d, c)\in D} \mu_c + \log \frac{1}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w\in V} q_{w, c}^{n_{w, d}}$$
                となる。
            </p>
            <p>したがって，この対数尤度を偏微分すると，$D_c$をクラスが$c$である訓練事例の集合であるものとして，
                $$\begin{align*}
                    \frac{\partial \log P(D)}{\partial \mu_c} &= \sum_{d\in D_c} \left(\sum_w n_{w, d} \right) \frac{1}{\mu_c} - N_c \\
                    &= \frac{1}{\mu_c} \sum_{d \in D_c} \left(\sum_w n_{w, d} \right) - N_c
                \end{align*}$$
                となる。
            </p>
            <p>よって，これが0と等しいとき，最大値が得られるから，
                $$\mu_c = \frac{\dis{\sum_{d \in D_c}}\left(\dis{\sum_w} n_{w, d} \right) }{N_c}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="4-2--2">
            <span class="def-t ">
                <h4>4.2.2 [2] 多項モデルのパラメータの最尤推定(p. 112)</h4>
            </span>
            <p>多項モデルについて，与えられたデータに対する最尤推定によりパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>最尤推定を行う際に最大化する目的関数は，
                $$\begin{align*} 
                    \log P(D) &= \sum_{(d, c) \in D} \log P(d,c) \\ 
                    &= \sum_{(d, c) \in D} \log \left( p_c \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}} \right) \\
                    &= \sum_{(d, c) \in D} \log  \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} + \sum_{(d, c) \in D} \log p_c + \sum_{(d, c) \in D} \sum_{w \in V} n_{w, d} \log q_{w, c} \\
                    &= \sum_{(d, c) \in D} \log  \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} + \sum_{c} N_c \log p_c + \sum_{c} \sum_{w \in V} n_{w, c} \log q_{w, c} 
                \end{align*}$$
                となる。
            </p>
            <p>したがって，これを最大化するため，ラグランジュ関数$L(\theta, \boldsymbol{\lambda_1}, \lambda_2)$を定義すると，
                $$L(\theta, \boldsymbol{\lambda_1}, \lambda_2) = \log P(D) + \sum_{c \in C} \lambda_{1c} \left(\sum_{w \in V} q_{w, c} - 1 \right) + \lambda_2 \left(\sum_{c \in C} p_c - 1 \right)$$
            </p>
            <p>よって，これを偏微分して，0と等しいものとすると，
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial q_{w, c}} = \frac{n_{w, c}}{q_{w, c}} + \lambda_{1c} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial p_c} = \frac{N_c}{p_c} + \lambda_{2} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_1} = \sum_{w \in V} q_{w, c} - 1 = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_2} = \sum_{c \in C} p_c - 1 = 0$$
                となる。
            </p>
            <p>この連立方程式を解くと，
                $$q_{w, c} = \frac{n_{w, c}}{\dis{\sum_{w}} n_{w, c}}$$
                $$p_c = \frac{N_c}{\dis{\sum_c} N_c}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="4-2--3">
            <span class="def-t ">
                <h4>4.2.2 [3] 多項モデルのパラメータのMAP推定(p. 115)</h4>
            </span>
            <p>ディリクレ分布を事前分布として用いた上で，多項モデルにおけるMAP推定を行うとどのようになるか説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>MAP推定を用いて最大化する際の目的関数は，
                $$\begin{align*}
                    \log P(\theta) + \log P(D) &= \log \left(\prod_c p_c^{\alpha - 1} \right) \times \left(\prod_{w, c} q_{w, c}^{\alpha - 1}\right) + \sum_{(d, c) \in D} \log P(d, c) + \mathrm{const.} \\
                    &= (\alpha - 1)\left(\sum_c \log p_c + \sum_{w, c} \log q_{w, c}\right) + \sum_{(d, c) \in D} \log \left( p_c \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}}  \right) 
                    \\ & \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  + \mathrm{const.}
                \end{align*}$$
            </p>
            <p>したがって，これを最大化するため，ラグランジュ関数$L(\theta, \boldsymbol{\lambda_1}, \lambda_2)$を定義すると，
                $$L(\theta, \boldsymbol{\lambda_1}, \lambda_2) = \log P(\theta) + \log P(D) + \sum_{c \in C} \lambda_{1c} \left(\sum_{w \in V} q_{w, c} - 1 \right) + \lambda_2 \left(\sum_{c \in C} p_c - 1 \right)$$
            </p>
            <p>よって，これを偏微分して，0と等しいものとすると，
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial q_{w, c}} = \frac{\alpha - 1}{q_{w, c}} + \frac{n_{w, c}}{q_{w, c}} + \lambda_{1c} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial p_c} = \frac{\alpha - 1}{p_c} + \frac{N_c}{p_c} + \lambda_{2} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_{1c}} = \sum_{w \in V} q_{w, c} - 1 = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_2} = \sum_{c \in C} p_c - 1 = 0$$
                となる。
            </p>
            <p>この連立方程式を解くと，
                $$q_{w, c} = \frac{n_{w, c} + \alpha - 1}{\dis{\sum_{w}} n_{w, c} + |W| (\alpha - 1)}$$
                $$p_c = \frac{N_c + \alpha - 1}{\dis{\sum_c} N_c + |C|(\alpha - 1)}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>章末問題</h3>
        <div class="def " id="2">
            <span class="def-t ">
                <h4>【 2 】</h4>
            </span>
           <p>多項モデルのナイーブベイズ分類器を少し変形することを考えよう。クラスが$c$であるような文書$d$において，単語$w$が$k$回出現する確率は，ポアソン分布$P(k; \mu_{w, c}) = \cfrac{\mu_{m, c}^k e^{-\mu_{m, c}}}{k!}$に従うとする。このようなモデルのナイーブベイズ分類器を作りたい。$P(d | c)$を求めよ。また，最尤推定を用いてパラメータを推定する式を求めよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$P(d|c)$は，
                $$P(d|c) = P\left(\sum_w n_{w, d}\right)\prod_w \frac{\mu_{w, c}^{n_{w, d}}}{n_{w, d}!} e^{-\mu_{w, c}}$$
            となる。</p>
            <p>対数尤度は，
                $$\begin{align*}
                    \log P(D) &= \sum_{d\in D} \log p_c P\left(\sum_w n_{w, d} \right) \prod_w \frac{\mu_{w, c}^{n_{w, d}}}{n_{w, d}!} e^{-\mu_{w, c}} \\
                    &= \sum_{d\in D} \log p_c P\left(\sum_w n_{w, d} \right) + \sum_c \sum_w n_{w, c}\log \mu_{w, c} - \sum_c N_c \sum_w \mu_{w, c} - \sum_c N_c \sum_w \log n_{w, c}!
                \end{align*}$$
                となる。
            </p>
            <p>これを$\mu_{w, c}$で偏微分して，
                $$\frac{\partial \log P(D)}{\partial \mu_{w, c}} = \frac{n_{w, c}}{\mu_{w, c}} - N_c$$
                となるから，これが0と等しいとき，最大値を取るので，
                $$\mu_{w, c} = \frac{n_{w, c}}{N_c}$$
            </p>

        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="3">
            <span class="def-t ">
                <h4>【 3 】</h4>
            </span>
            <p>SVMにおける最適化問題
                $$\min. \; \frac{1}{2} \bm{w}^2$$
                $$\mathrm{s.t. }\; y^{(i)} (\bm{w} \cdot \bm{x}^{(i)} - b) - 1 \geqq 0; \forall i$$
                は下に凸な凸計画問題であることを示せ。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>目的関数$\frac{1}{2}\boldsymbol{w}^2=\dis{\sum_i}\frac{1}{2} w_i^2$は，第1章における章末問題【5】の内容である，<br>
                $f(x)=ax2$とすると，$f′(x)=2ax$, $f′′(x)=2a$となり，$a\geqq0 \Longrightarrow ax^2は下に凸$ <br>
                より，上に凸であるといえる。
            </p>
            <p>また，<a href="./note1.html#1">第1章 章末問題【1】, 【2】</a>の内容における凸関数であることの証明を適用することで，本問においても凸関数であることが明らかである。</p>
        </div>

        <div class="def " id="4">
            <span class="def-t ">
                <h4>【 4 】</h4>
            </span>
            <p>訓練データ$D = \{(\bm{x}^{(1)}, -1), (\bm{x}^{(2)}, 1), (\bm{x}^{(3)}, 1)\}$に対し，SVMを構築せよ。ただし，$\bm{x}^{(1)} = \left(0, \cfrac{1}{3} \right), \bm{x}^{(2)} = (1, 1), \bm{x}^{(3)} = (1, 0)$であるとする。</p>
        </div>

        <div class="th">
            <span class="th-t">
                <h5>方針</h5>
            </span>
            <p>厳密制約下でSVMモデルを考えるとき，次のような最適化問題を解けばよい。
                $$\min . \quad \frac{1}{2} \boldsymbol{w}^2 $$
                $$\mathrm{s,t,} \quad y^{(i)} (\bm{w} \cdot \bm{x}^{(i)} - b) - 1 \geqq 0; \forall i$$
            </p>
            <p>ラグランジュ関数を定義して，各変数において偏微分した式を０とおくことで，次の式が得られる。
                $$\boldsymbol{w}^* = \sum_i \alpha_i y^{(i)} \boldsymbol{x}^{(i)}$$
                $$\sum_i \alpha_i y^{(i)} = 0$$
            </p>
            <p> $\boldsymbol{w}^* = \dis{\sum_i} \alpha_i y^{(i)} \boldsymbol{x}^{(i)}$を分離平面の式$\boldsymbol{w}\cdot \boldsymbol{x} = b$に代入すると，
                $$f(\boldsymbol{x}) = \sum_i \alpha_i y^{(i)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x} - b$$
                が得られるから，この等式をもとにしき変形することで，
                $$L(\boldsymbol{x}^*,\; b^*,\; \alpha) = -\frac{1}{2} \dis{\sum_{i = 1}^3} \dis{\sum_{j = 1}^3} \alpha_i \alpha_j y^{(i)} y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} + \sum_{i = 1}^3 \alpha_i $$
                が得られる。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>まず，$\dis{\sum_{i = 1}^3} \dis{\sum_{j = 1}^3} \boldsymbol{x}^{(i)} \boldsymbol{x}^{(j)}$における各項をそれぞれ計算すると，
                $$\boldsymbol{x}^{(1)} \cdot \boldsymbol{x}^{(1)} = \frac{1}{9},\quad \boldsymbol{x}^{(1)} \cdot \boldsymbol{x}^{(2)} = \frac{1}{3} \quad \boldsymbol{x}^{(1)} \cdot \boldsymbol{x}^{(3)} = 0 $$
                $$\boldsymbol{x}^{(2)} \cdot \boldsymbol{x}^{(1)} = \frac{1}{3},\quad \boldsymbol{x}^{(2)} \cdot \boldsymbol{x}^{(2)} = 2 \quad \boldsymbol{x}^{(2)} \cdot \boldsymbol{x}^{(3)} = 1 $$
                $$\boldsymbol{x}^{(3)} \cdot \boldsymbol{x}^{(1)} = 0,\quad \boldsymbol{x}^{(3)} \cdot \boldsymbol{x}^{(2)} = 1 \quad \boldsymbol{x}^{(3)} \cdot \boldsymbol{x}^{(3)} = 1 $$
                となる。
            </p>
            <p>双対ラグランジュ関数は，
                $$\begin{align*} 
                    L(\boldsymbol{x}^*,\; b^*,\; \alpha) &= -\frac{1}{2} \dis{\sum_{i = 1}^3} \dis{\sum_{j = 1}^3} \alpha_i \alpha_j y^{(i)} y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} + \sum_{i = 1}^3 \alpha_i \\
                    &= - \frac{1}{2} (\alpha_1^2 \times (-1) \times (-1) \times \frac{1}{9} + \alpha_2^2 \times 1 \times 1 \times 2 + \alpha_3^2 \times 1 \times 1 \times 1\\ 
                    &\quad + \alpha_1 \alpha_2 \times (-1) \times 1 \times \frac{1}{3} + \alpha_2 \alpha_1 \times 1 \times (-1) \times \frac{1}{3} + \alpha_2 \alpha_3 \times 1 \times 1 \times 1 \\
                    &\quad + \alpha_3 \alpha_2 \times 1 \times 1 \times 1 + \alpha_1 \alpha_3 \times (-1) \times 1 \times 0 + \alpha_3 \alpha_1 \times 1 \times (-1) \times 0 \left.  \right) \\
                    &\quad + \alpha_1 + \alpha_2 + \alpha_3 \\
                    &= \frac{1}{18} \alpha_1^2 - \alpha_2^2 - \frac{1}{2}\alpha_3^2 - \frac{1}{2} \alpha_3^2 + \frac{1}{3}\alpha_1 \alpha_2 - \alpha_2 \alpha_3 + \alpha_1 + \alpha_2 + \alpha_3
                \end{align*}$$
            </p>
            <p>ここで，$\dis{\sum_i} \alpha_i y^{(i)} = 0$より，$-\alpha_1 + \alpha_2 + \alpha_3 = 0$が成り立つから，これを利用して$\alpha_3$を消去すると，
                $$L(\boldsymbol{x}^*,\; b^*,\; \alpha) = -\frac{5}{9}\alpha_1^2 - \frac{1}{2} \alpha_2^2 + \frac{1}{3} \alpha_1 \alpha_2 + 2 \alpha_1$$
                となる。
            </p>
            <p>次に，$\alpha_1, \alpha_2$について，それぞれ偏微分すると，
                $$\frac{\partial L(\boldsymbol{x}^*,\; b^*,\; \alpha)}{\partial \alpha_1} = - \frac{10}{9} \alpha_1 + \frac{1}{3} \alpha_2 + 2$$
                $$\frac{\partial L(\boldsymbol{x}^*,\; b^*,\; \alpha)}{\partial \alpha_2} = \frac{1}{3} \alpha_1 - \alpha_2$$
            </p>
            <p>したがって，それぞれの式が0と等しいとき，最大化されることから，
                $$\begin{pmatrix} - \cfrac{10}{9} & \cfrac{1}{3} \\ \cfrac{1}{3} & -1\end{pmatrix} \begin{pmatrix} \alpha_1 \\ \alpha_2 \end{pmatrix} = \begin{pmatrix} -2 \\ 0 \end{pmatrix}$$
                $$\therefore \begin{pmatrix} \alpha_1 \\ \alpha_2 \end{pmatrix} = \begin{pmatrix} 2 \\ \cfrac{2}{3} \end{pmatrix}$$
            </p>
            <p>$-\alpha_1 + \alpha_2 + \alpha_3 = 0$より，$\alpha_3 = \cfrac{4}{3}$</p>
            <p>したがって，
                $$\begin{align*}
                    \boldsymbol{w} &= 2 \times (-1) \times \boldsymbol{x}^{(1)} + \frac{2}{3} \times 1 \times \boldsymbol{x}^{(2)} + \frac{4}{3} \times 1 \times \boldsymbol{x}^{(3)}\\
                    &= \left(0 + \frac{2}{3} + \frac{4}{3},\; -\frac{1}{3} \cdot 2 + \frac{2}{3} + 0 \right) = (2, 0)
                \end{align*}$$
                $$b = (2, 0) \cdot (1, 1) - 1 = 2 - 1 = 1$$
            </p>
            <p>よって，判別関数は次のようになる。
                $$f(\boldsymbol{x}) = (2, 0)\cdot \boldsymbol{x} - 1$$
            </p>

        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="rei">
            <span class="def-t ">
                <h4>例題4.11</h4>
            </span>
            <p>例題4.9のデータ$D = \{(x^{(1)},\; -1),\; (x^{(2)},\; 1) \}$に対して，2次の多項式カーネル$K_{poly, 2}$を用いてSVMを構築せよ。ただし，$\boldsymbol{x}^{(1)} = (0, 1), \boldsymbol{x}^{(2)} = (1, 1)$である。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>
                $$K_{poly, 2} (\bx{1}, \bx{1}) = (\bx{1} \cdot \bx{1} + 1)^2 = 4$$
                $$K_{poly, 2} (\bx{1}, \bx{2}) = (\bx{1} \cdot \bx{2} + 1)^2 = 4$$
                $$K_{poly, 2} (\bx{2}, \bx{2}) = (\bx{2} \cdot \bx{2} + 1)^2 = 9$$
                となる。
            </p>
            <p>双対ラグランジュ関数は，
                $$\begin{align*}
                    L(\bm{w}^*, b, \alpha_1, \alpha_2) &= -\frac{1}{2} (\alpha_1 \alpha_1 \times (-1) \times (-1) \times 4 + \alpha_1 \alpha_2 \times (-1) \times 1 \times 4 \\ 
                        &\qquad + \alpha_2 \alpha_1 \times 1 \times (-1) \times 4 + \alpha_2 \alpha_2 \times 1 \times 9) + \alpha_1 + \alpha_2 \\
                        &= - 2\alpha_1^2 - \frac{9}{2} \alpha_2^2 + 4 \alpha_1 \alpha_2 + \alpha_1 + \alpha_2
                \end{align*}$$
                となる。
            </p>
            <p>また，$\alpha_1 \times (-1) + \alpha_2 \times 1 = 0$となるから，
                $$L(\bm{w}^*, b, \alpha_1, \alpha_2) = - \frac{5}{2} \alpha_1^2 + 2 \alpha_1$$
                となる。
            </p>
            <p>したがって，これを$\alpha_1$で微分して，0と等しいものとすると，
                $$\frac{d L(\bm{w}^*, b, \alpha_1, \alpha_2)}{d \alpha_1} = - 5\alpha_1 + 2 = 0$$
                $$\therefore \alpha_1 = \frac{2}{5}$$
                となり，このとき最大化されるから，これを$-\alpha_1 + \alpha_2 = 0$の式に代入して，
                $$\alpha_1 = \alpha_2 = \frac{2}{5}$$
                となる。
            </p>
            <p>また，$\bm{x}_+ = (1, 1)$を用いると，
                $$ b = \frac{2}{5} \times (-1) \times K((0, 1), (1, 1)) + \frac{2}{5} \times 1 \times K((1, 1), (1, 1)) - 1 = 2 - 1 = 1$$
                となる。
            </p>
            <p>さらに，$f(\bm{x})$を求めると，
                $$\begin{align*}
                    f(\bm{x}) &= \frac{2}{5} \times (-1) \times K((0, 1), \bm{x}) + \frac{2}{5} \times 1 \times K((1, 1), \bm{x}) - 1\\
                        &= \frac{2}{5} x_1^2 + \frac{4}{5} x_1 x_2 + \frac{4}{5} x_1 - 1
                \end{align*}$$
                となり，分離曲面は，$2x_1^2 + 4x_1 x_2 + 4 x_1 = 5$であることが分かる。
            </p>

        </div>

        <h3>章末問題</h3>
        <div class="def " id="5">
            <span class="def-t ">
                <h4>【 5 】</h4>
            </span>
            <p>緩和制約下のSVMについて，そのパラメータ$b$は，$0 \lt a_i \lt C$なる事例を一つ持って来て$b = \bm{w} \cdot \bm{x}^{(i)} - y^{(i)}$とすることで計算できることを示せ。KKT条件(A. 3)を用いればよい。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$\alpha_i \lt C$とすると，$\beta_i = C - \alpha_i \gt 0$となる。</p>
            <p>このとき，KKT条件より，$\xi_i = 0$である。</p>
            <p>さらに，$\alpha_i \gt 0$であるとき，KKT条件より，
                $$y^{i} (\bm{w} \cdot \bx{i} - b) - 1 + \xi_i = 0$$
                である。
            </p>
            <p>よって，この式に対して，$\xi_i = 0$を用いることで，
                $$b = \bm{w} \cdot \bx{i} - y^{(i)}$$
                と計算できる。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="6">
            <span class="def-t ">
                <h4>【 6 】</h4>
            </span>
            <p>ラベル付きデータ$D^l$とラベル無しデータ$D^u$が与えられたとする。
                $$\log P(D^l)P(D^u) = \sum_{(d, c) \in D^l} \log P(d, c) + \sum_{d \in D^u} \log P(d)$$
                がデータの対数尤度である。EMアルゴリズムを用いて，これらのデータから多項モデルのナイーブベイズ分類器を構築せよ。クラス変数が隠れ変数となる($Q$関数を作る際，隠れ変数に関する期待値は$D^u$内の事例についてのみとればよい)。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>Eステップは，$(d, c) \in D^l$について，$P(c | d; \theta') = 1$とし，それ以外のクラス$c'$においては，$P(c'|d; \theta') = 0$とする。</p>
            <p>また，$d\in D^u$においては，
                $$P(c'|d; \theta') = \frac{p'_c \dis{\prod_{w \in V}} q_{w, c}^{'n_{w, d}} }{\dis{\sum_c} p'_c \dis{\prod_{w \in V}} q_{w, c}^{'n_{w, d}}}$$
                となる。
            </p>
            <p>Mステップにおいて，Q関数は，
                $$\begin{align*}
                    Q(\theta; \theta') &= \sum_{(d, c) \in D^l} \log P(d, c) + \sum_{d \in D^u} \sum_c P(c | d; \theta') \log P(d, c) \\
                    &= \sum_{(d, c) \in D^l} \log p_c + \sum_{(d, c) \in D^l} \sum_{w \in V} n_{w, d} \log q_{w, c} + \sum_{d \in D^u} \sum_c P(c | d; \theta') \log p_c \\
                    &\qquad \qquad + \sum_{d \in D^u} \sum_c P(c | d; \theta') \sum_{w \in V} n_{w, d} \log q_{w, c}
                \end{align*}$$
            </p>
            <p>ラグランジュ関数は，
                $$L(\theta, \bm{\lambda_{1c}}, \lambda_2) = Q(\theta, \theta') + \sum_{c \in C} + \sum_{c \in C} \lambda_{1c} \left(\sum_{w \in V} q_{w, c} - 1 \right) + \lambda_2 \left(\sum_{c \in C}p_c - 1 \right)$$
                となる。
            </p>
            <p>よって，ラグランジュ関数を$q_{w, c}$について偏微分すると，
                $$\frac{\partial L(\theta, \bm{\lambda_1}, \lambda_2)}{\partial q_{w, c}} = \sum_{d \in D_c^l} \frac{n_{w, d}}{q_{w, c}} + \sum_{d \in D^u} P(c | d; \theta') \frac{n_{w, d}}{q_{w, c}} + \lambda_{1c}$$
                となる。
            </p>
            <p>したがって，これを0と等しいものとして，$\dis{\sum_{w \in V}} q_{w, c} - 1 = 0$と合わせて連立方程式を解くことで，次のようにして解ける。</p>
            <p>$$q_{w, c} = \frac{\dis{\sum_{d \in D_c^l}} n_{w, d} + \dis{\sum_{d \in D^u}} P(c | d; \theta') n_{w, d}}{\dis{\sum_w} \left(\dis{\sum_{d \in D_c^l}} n_{w, d} + \dis{\sum_{d \in D^u}} P(c | d; \theta') n_{w, d} \right)}$$</p>
            <p>また，$p_c$についても偏微分して，0とおき，$\dis{\sum_{c \in C}} p_c - 1 = 0$と合わせて連立方程式を解くことで，次のようにして解ける。</p>
            <p>$$p_c = \frac{|D_c^l| + \dis{\sum_d \in D^u} P(c|d; \theta')}{\dis{\sum_c} \left(|D_c^l| + \dis{\sum_{d \in D^u}} P(c | d; \theta') \right)}$$</p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="4-5-2">
            <span class="def-t ">
                <h4>4.5.2 対数線形モデルの更新式の導出</h4>
            </span>
            <p>対数線形モデルにおける目的関数を考え，目的関数に対して最急勾配法を用いた際の更新式を導出せよ。</p>
        </div>

        <div class="th">
            <span class="th-t">
                <h5>方針</h5>
            </span>
            <p>対数線形モデルについて軽くおさらいする。</p>
            <p>まず，<em>素性関数(feature function)</em>$\phi_k$を定義する。
            これは，事例$d$とラベル$y$を引数にとって，素性値$\phi_k(d, y)$を返すものであり，これを並べた$\boldsymbol{\phi}(d, y)$を素性ベクトルとする。</p>
            <p>次に，分類器として求めたい$P(y|d)$を表すにあたって，素性に対する重みベクトル$\boldsymbol{w}$，$\dis{\sum_y P(y|d) = 1}$を保証する係数$Z_{d, w} = \dis{\sum_y \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d, y))}$を定義すると，$P(y|d)$は次のように表される。</p>
            <p>$$\begin{align*}
                    P(y|d) &= \frac{1}{Z_{d, w}} \exp(\boldsymbol{w}\cdot \boldsymbol{\phi}(d, y)) \\
                    &= \frac{\exp(\boldsymbol{w}\cdot \boldsymbol{\phi}(d, y))}{\dis{\sum_y \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d, y))}} 
                \end{align*}$$</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>方針の続きを本問題の解答とする。</p>
            <p>訓練データ$D$が与えられた際に，パラメータ推定を行うこととすると，$P(y|d)$の対数尤度は，
                $$\begin{align*} 
                    \log P_{\mathrm{cond}}(D) &= \sum_{(d^{(i)}, y^{(i)}) \in D} \log P(y^{(i)}| d^{(i)}) \\
                    &= \sum_{(d^{(i)}, y^{(i)}) \in D} \log \frac{1}{Z_{d, w}} \exp(\boldsymbol{w}\cdot \boldsymbol{\phi}(d^{(i)}, y^{(i)})) \\
                    &= \sum_{(d^{(i)}, y^{(i)}) \in D} (\log \exp(\boldsymbol{w}\cdot \boldsymbol{\phi}(d^{(i)}, y^{(i)})) - \log Z_{d^{(i)}, \boldsymbol{w}} ) \\
                    &= \sum_{(d^{(i)}, y^{(i)}) \in D} (\boldsymbol{w}\cdot \boldsymbol{\phi}(d^{(i)}, y^{(i)}) - \log Z_{d^{(i)}, \boldsymbol{w}} ) \\
                    &= \sum_{(d^{(i)}, y^{(i)}) \in D} \left(\boldsymbol{w}\cdot \boldsymbol{\phi}(d^{(i)}, y^{(i)}) - \log \sum_y \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d^{(i)}, y)) \right)
                \end{align*}$$
                と表される。
            </p>
            <p>ただし，これを目的関数にすると，あるクラスだけに出現した素性に対する重みが無限大になることから，正規化を行い，次のような目的関数が得られる。</p>
            <p>$$L(\boldsymbol{w}) =  \sum_{(d^{(i)}, y^{(i)}) \in D}  \log P(y^{(i)} | d^{(i)}) - \frac{C}{2} |\boldsymbol{w}|^2 $$</p>
            <p>ただし，この目的関数は解析的に最大化できないことから，最大化されるパラメータ$\boldsymbol{w}$を求めるにあたって，最急勾配法を用いて，更新式を繰り返すことで収束に導く手法を用いることとする。</p>
            <p>このとき，最急勾配法における更新式は，
                $$\boldsymbol{w}^{new} = \boldsymbol{w}^{old} + \epsilon \nabla_w L(\boldsymbol{w}^{old})$$
                と表されるから，今回の$L(\boldsymbol{w})$に対応する$\nabla_w L(\boldsymbol{w})$を求めると，
                $$\begin{align*}
                    \nabla_w L(\boldsymbol{w}) &= \frac{\partial}{\partial \boldsymbol{w}} \left( \sum_{(d^{(i)}, y^{(i)}) \in D}  \log P(y^{(i)} | d^{(i)}) - \frac{C}{2} |\boldsymbol{w}|^2  \right) \\
                    &=  \frac{\partial}{\partial \boldsymbol{w}} \left( \sum_{(d^{(i)}, y^{(i)}) \in D}  \left( \boldsymbol{w}\cdot \boldsymbol{\phi}(d^{(i)}, y^{(i)}) - \log \sum_y \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d^{(i)}, y)) \right) - \frac{C}{2} |\boldsymbol{w}|^2  \right) \\
                    &=  \sum_{(d^{(i)}, y^{(i)}) \in D}  \left(  \boldsymbol{\phi}(d^{(i)}, y^{(i)}) -  \frac{\sum_y  \boldsymbol{\phi}(d^{(i)}, y) \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d^{(i)}, y))}{\sum_y \exp(\boldsymbol{w} \cdot \boldsymbol{\phi}(d^{(i)}, y))} \right) - \frac{C}{2} |\boldsymbol{w}|^2  \\
                    &=  \sum_{(d^{(i)}, y^{(i)}) \in D}  \left(  \boldsymbol{\phi}(d^{(i)}, y^{(i)}) - \sum_y P(y|d^{(i)}) \boldsymbol{\phi}(d^{(i)}, y)  \right) - C \boldsymbol{w}  
                \end{align*}$$
                となる。
            </p>
            <p>したがって，更新式は，
                $$\begin{align*}
                    \boldsymbol{w}^{new} &= \boldsymbol{w}^{old} + \epsilon \nabla_w L(\boldsymbol{w}^{old}) \\
                    &= \boldsymbol{w}^{old} + \epsilon \sum_{(d^{(i)}, y^{(i)}) \in D}  \left(  \boldsymbol{\phi}(d^{(i)}, y^{(i)}) - \sum_y P(y|d^{(i)}) \boldsymbol{\phi}(d^{(i)}, y)  \right) - \epsilon C \boldsymbol{w} 
                \end{align*}$$
                となる。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="flex">
            <p style="width:33%;"><a href="./note3.html " class="btn-square-slant ">←前のページに移る</a></p>
            <p style="width:33%;"><a href="../index.html " class="btn-square-slant ">トップページに戻る</a></p>
            <p style="width:33%;"><a href="./note5.html " class="btn-square-slant ">次のページに移る→</a></p>
        </div>

        <br>

</body>

</html>