<!DOCTYPE html>
<!-- saved from url=(0087)file:///C:/Users/ISE/Desktop/shindai%20programming%20language%20I%20homework%203-2.html -->
<html lang="ja">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="../notestyle.css">
    <link rel="stylesheet" href="../button.css">
    <title>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート4</title>
     <!--
    <script type="text/javascript" src="./config.js" defer></script>
    TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    N: "{\\mathrm{N}}",
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\displaystyle{\mathrm{max}}}"
                },
                extensions: [
                    "cancel.js",           // 抹消線を有効にする。
                    "color.js"             // 文字の色付けを有効にする。
                ]
            },
    -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                Macros:{
                    dis: ["{\\displaystyle{#1}}",1],
                    bm:["{\\boldsymbol{#1}}",1],
                    dlim: ["{\\displaystyle{\\lim_{#1}}}",1],
                    Lim: ["{\\displaystyle{\\lim_{#1 \\to \\infty}}}",1],
                    ep: "{\\varepsilon}",
                    max: "{\\mathrm{max}}",
                    N: "{\\mathbb{N}}",
                    R: "{\\mathbb{R}}",
                    min: "{\\mathrm{min}}",
                    tx: "{\\tilde{x}}",
                },
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\"] ],
                processEscapes: true
            }, 
            "HTML-CSS": {
                matchFontHeight: false,
                undefinedFamily: "Meiryo, STIXGeneral, 'Arial Unicode MS', serif",
                mtextInheritFont: true,
                scale: 100
            },
            displayAlign: "left",
            displayIndent: "4em"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


<body>


    <div class="document" id="top">
        <h1><span>自然言語処理シリーズ 1 言語処理のための機械学習入門 ノート4</span></h1>
        <p style="text-align:right;">電気通信大学大学院 情報理工学研究科 情報学専攻1年 鈴木一生(2430073)</p>
        <h2>リンク集</h2>
        <div class="th" id="link">
            <span class="th-t" style="width:10em;">
                <h5>ページ構成(リンク)</h5>
            </span>
            <ul>
                <li><a href="#forth">第4章 分類</a></li>
                <ul>
                    <li><a href="#4--2">4.2.1 [2] 多変数ベルヌーイモデルのパラメータの最尤推定</a></li>
                    <li><a href="#4--3">4.2.1 [3] 多変数ベルヌーイモデルのパラメータのMAP推定</a></li>
                </ul>
                <li><a href="#shomatsu">章末問題</a></li>
                <ul>
                    <li><a href="#1">【1】</a></li>
                </ul>
                <li><a href="">教科書</a></li>
                <ul>
                    <li><a href="#4-2--2">4.2.2 [2] 多項モデルのパラメータの最尤推定</a></li>
                    <li><a href="#4-2--3">4.2.2 [3] 多項モデルのパラメータのMAP推定</a></li>
                </ul>
                <li><a href="#shomatsu2">章末問題</a></li>
                <ul>
                    <li><a href="#2">【2】</a></li>
                </ul>
                <ul>
                    <li><a href="#3">【3】</a></li>
                </ul>
                <ul>
                    <li><a href="#4">【4】</a></li>
                </ul>
                <li><a href="#rei">教科書</a></li>
                <ul>
                    <li><a href="#rei">例題4.11</a></li>
                </ul>
                <li><a href="">章末問題</a></li>
                <ul>
                    <li><a href="#5">【5】</a></li>
                </ul>
                <ul>
                    <li><a href="#6">【6】</a></li>
                </ul>
                <li><a href="">教科書</a></li>
                <ul>
                    <li><a href="#4-5-2">4.5.2 対数線形モデルの更新式の導出</a></li>
                </ul>
            </ul>
        </div>


        <h2 id="forth">第4章 分類</h2>
        <h3>教科書</h3>
        <div class="def " id="4--2">
            <span class="def-t ">
                <h4>4.2.1 [2] 多変数ベルヌーイモデルのパラメータの最尤推定(p. 103)</h4>
            </span>
            <p>最尤推定を用い，与えられたデータからパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>教科書103ページの式(4.6)に示されている，ナイーブベイズ分類器における多変数ベルヌーイモデルのパラメータにおける最尤推定であるから，
                $$\begin{align*} \log P(D) &= \sum_{(d, c) \in D} \log P(d, c) \\ &= \sum_{(d, c) \in D} \log \left(p_c \prod_{w\in V} \left(p_{w, c}^{\delta_{w, d}} (1 - p_{w, c})^{1 - \delta_{w, d}} \right) \right) \\ &=  \sum_{(d, c) \in D} \left(\log  p_c + \sum_{w\in V} \left( \delta_{w, d}\log p_{w, c} + (1 - \delta_{w, d}) \log (1 - p_{w, c}) \right) \right)\end{align*}$$
            </p>
            <p>ここで，
                $$N_{w, c}: \text{クラス$c$であり，かつ$w$を含むような訓練文書の数}$$
                $$N_{c}: \text{クラス$c$であるような訓練文書の数}$$
                と表すことにすると，$d$に関する総和を考えて，次のようになる。
            </p>
            <p>$$\log P(D) = \sum_{c} N_c \log  p_c + \sum_{c} \sum_{w\in V}  N_{w, c}\log p_{w, c} + \sum_{c} (N_c - N_{w, c}) \log (1 - p_{w, c})  $$</p>
            <p>したがって，この$\log P(D)$を制約条件$\dis{\sum_c p_c = 1}$のもとで，最大化すればよいので，ラグランジュ関数$L(p_{w, c}, \; p_c,\; \lambda)$は，
                $$L(p_{w, c}, \; p_c,\; \lambda) = \sum_{c} N_c \log  p_c + \sum_{c} \sum_{w\in V}  N_{w, c}\log p_{w, c} \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  + \sum_{c} (N_c - N_{w, c}) \log (1 - p_{w, c}) + \lambda \left(\sum_c p_c - 1 \right)$$
            </p>
            <p>よって，このラグランジュ関数$L(p_{w, c}, \; p_c,\; \lambda)$を各変数で偏微分した式それぞれが0に等しくなるような変数の値が最大値となるから，
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial p_{w, c}} = \frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0 \cdots \cdots ①$$
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial p_{c}} = \frac{N_c}{p_c} + \lambda = 0 \cdots \cdots ➁$$
                $$\frac{\partial L(p_{w, c}, \; p_c,\; \lambda)}{\partial \lambda} = \sum_c p_c - 1 = 0 \cdots \cdots ③$$</p>
            <p>①より，$\frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0 $に対して，両辺$p_{w, c} (1 - p_{w, c})$をかけて，
                $$N_{w, c} - N_{w, c} p_{w, c} = N_c p_{w, c} - N_{w, c} p_{w, c} $$
                $$\therefore p_{w, c} = \frac{N_{w, c}}{N_c}$$
            </p>
            <p>➁より，$p_c = - \cfrac{N_c}{\lambda}$となるから，これを③に代入して，
                $$\sum_c - \cfrac{N_c}{\lambda} = 1 \qquad \therefore \lambda = - \sum_{c} N_c$$
            </p>
            <p>よって，$p_c = \cfrac{N_c}{\dis{\sum_c} N_c}$となる。</p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="4--3">
            <span class="def-t ">
                <h4>4.2.1 [3] 多変数ベルヌーイモデルのパラメータのMAP推定(p. 107)</h4>
            </span>
            <p>MAP推定を用い，与えられたデータからパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>MAP推定を行うにあたって，ディリクレ分布を事前分布に用いることにして，目的関数となるものを計算すると，
                $$\begin{align*} 
                    \log P(\theta) + \log P(D) &= \log \left(\prod _c p_c^{\alpha - 1} \right) \times \left(\prod_{w, c} \left(p_{w, c}^{\alpha - 1} (1 - p_{w, c})^{\alpha - 1} \right)\right) + \sum_{(d, c) \in D} \log P(d|c) + \mathrm{const.} \\
                    &= (\alpha - 1)\sum_c \log p_c + (\alpha - 1)\sum_{w, c}\left(\log p_{w, c} + \log(1 - p_{w, c}) \right) \\ &\qquad \qquad \qquad \qquad  + \sum_{(d, c) \in D} \log \left(p_c \prod_{w \in V} \left(p_{w, c}^{\delta_{w, c}}(1 - p_{w, c})^{1 - \delta_{w, d}} \right) \right) + \mathrm{const.}
                \end{align*}$$
                となる。
            </p>
            <p>したがって，これを最大化するから，ラグランジュの未定乗数法を用いる。</p>
            <p>ラグランジュ関数$L(\theta, \lambda)$は，
                $$L(\theta, \lambda) = \log P(\theta) + \log P(D) + \lambda \left(\sum_c p_c - 1 \right)$$
                となる。
            </p>
            <p>よって，このラグランジュ関数を各変数で偏微分して，それぞれの式が0と等しいものとして，連立方程式を解くことで最大化できるから，ラグランジュ関数を各変数で偏微分して，
                $$\frac{\partial L(\theta, \lambda)}{\partial p_{w, c}} = \frac{\alpha - 1}{p_{w, c}} + \frac{\alpha - 1}{1 - p_{w, c}} + \frac{N_{w, c}}{p_{w, c}} - \frac{N_c - N_{w, c}}{1 - p_{w, c}} = 0$$
                $$\frac{\partial L(\theta, \lambda)}{\partial p_c} = \frac{\alpha - 1}{p_c} + \frac{N_c}{p_c} + \lambda = 0$$
                $$\frac{\partial L(\theta, \lambda)}{\partial \lambda} = \sum_c p_c - 1 = 0$$
                となる。
            </p>
            <p>したがって，これらを解くと，
                $$p_{w, c} = \frac{N_{w, c} + \alpha - 1}{N_c + 2(\alpha - 1)}$$
                $$p_c = \frac{N_c + \alpha - 1}{\dis{\sum_c} N_c + |C| (\alpha - 1)}\qquad (|C|\text{はクラス数を示す})$$
                となる。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>章末問題(p. 145)</h3>
        <div class="def " id="1">
            <span class="def-t ">
                <h4>【 1 】</h4>
            </span>
            <p>文書の長さがクラスに依存するような，多項モデルのナイーブベイズ分類器を作ってみる。文書の長さは(クラスに依存する)ポアソン分布に従うとして，$P(d|c)$を式で表せ。また，最尤推定を用いてパラメータを算出するための式を求めよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>ポアソン分布に従うことから，$P(d|c)$は，
                $$\begin{align*}
                P(d|c) &= \frac{\mu_c^{\dis{\sum_w} n_{w, d}}}{\left(\dis{\sum_w} n_{w, d} \right)!} e^{- \mu_c} \frac{\left(\dis{\sum_w}n_{w, d} \right)!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}} \\
                &= \mu_c^{\dis{\sum_w} n_{w, d}} e^{-\mu_c} \frac{1}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w\in V} q_{w, c}^{n_{w, d}}
                \end{align*}$$
                となる。
            </p>
            <p>ここで，対数尤度は，
                $$\log P(D) = \sum_{(d, c) \in D} \left(\sum_w n_{w, d} \right) \log \mu_c - \sum_{(d, c)\in D} \mu_c + \log \frac{1}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w\in V} q_{w, c}^{n_{w, d}}$$
                となる。
            </p>
            <p>したがって，この対数尤度を偏微分すると，$D_c$をクラスが$c$である訓練事例の集合であるものとして，
                $$\begin{align*}
                    \frac{\partial \log P(D)}{\partial \mu_c} &= \sum_{d\in D_c} \left(\sum_w n_{w, d} \right) \frac{1}{\mu_c} - N_c \\
                    &= \frac{1}{\mu_c} \sum_{d \in D_c} \left(\sum_w n_{w, d} \right) - N_c
                \end{align*}$$
                となる。
            </p>
            <p>よって，これが0と等しいとき，最大値が得られるから，
                $$\mu_c = \frac{\dis{\sum_{d \in D_c}}\left(\dis{\sum_w} n_{w, d} \right) }{N_c}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="4-2--2">
            <span class="def-t ">
                <h4>4.2.2 [2] 多項モデルのパラメータの最尤推定(p. 112)</h4>
            </span>
            <p>多項モデルについて，与えられたデータに対する最尤推定によりパラメータをどのように推定するかを説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>最尤推定を行う際に最大化する目的関数は，
                $$\begin{align*} 
                    \log P(D) &= \sum_{(d, c) \in D} \log P(d,c) \\ 
                    &= \sum_{(d, c) \in D} \log \left( p_c \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}} \right) \\
                    &= \sum_{(d, c) \in D} \log  \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} + \sum_{(d, c) \in D} \log p_c + \sum_{(d, c) \in D} \sum_{w \in V} n_{w, d} \log q_{w, c} \\
                    &= \sum_{(d, c) \in D} \log  \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} + \sum_{c} N_c \log p_c + \sum_{c} \sum_{w \in V} n_{w, c} \log q_{w, c} 
                \end{align*}$$
                となる。
            </p>
            <p>したがって，これを最大化するため，ラグランジュ関数$L(\theta, \boldsymbol{\lambda_1}, \lambda_2)$を定義すると，
                $$L(\theta, \boldsymbol{\lambda_1}, \lambda_2) = \log P(D) + \sum_{c \in C} \lambda_{1c} \left(\sum_{w \in V} q_{w, c} - 1 \right) + \lambda_2 \left(\sum_{c \in C} p_c - 1 \right)$$
            </p>
            <p>よって，これを偏微分して，0と等しいものとすると，
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial q_{w, c}} = \frac{n_{w, c}}{q_{w, c}} + \lambda_{1c} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial p_c} = \frac{N_c}{p_c} + \lambda_{2} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_1} = \sum_{w \in V} q_{w, c} - 1 = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_2} = \sum_{c \in C} p_c - 1 = 0$$
                となる。
            </p>
            <p>この連立方程式を解くと，
                $$q_{w, c} = \frac{n_{w, c}}{\dis{\sum_{w}} n_{w, c}}$$
                $$p_c = \frac{N_c}{\dis{\sum_c} N_c}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="4-2--3">
            <span class="def-t ">
                <h4>4.2.2 [3] 多項モデルのパラメータのMAP推定(p. 115)</h4>
            </span>
            <p>ディリクレ分布を事前分布として用いた上で，多項モデルにおけるMAP推定を行うとどのようになるか説明せよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>MAP推定を用いて最大化する際の目的関数は，
                $$\begin{align*}
                    \log P(\theta) + \log P(D) &= \log \left(\prod_c p_c^{\alpha - 1} \right) \times \left(\prod_{w, c} q_{w, c}^{\alpha - 1}\right) + \sum_{(d, c) \in D} \log P(d, c) + \mathrm{const.} \\
                    &= (\alpha - 1)\left(\sum_c \log p_c + \sum_{w, c} \log q_{w, c}\right) + \sum_{(d, c) \in D} \log \left( p_c \frac{P(|d|) |d|!}{\dis{\prod_{w \in V}} n_{w, d}!} \prod_{w \in V} q_{w, c}^{n_{w, d}}  \right) 
                    \\ & \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  + \mathrm{const.}
                \end{align*}$$
            </p>
            <p>したがって，これを最大化するため，ラグランジュ関数$L(\theta, \boldsymbol{\lambda_1}, \lambda_2)$を定義すると，
                $$L(\theta, \boldsymbol{\lambda_1}, \lambda_2) = \log P(\theta) + \log P(D) + \sum_{c \in C} \lambda_{1c} \left(\sum_{w \in V} q_{w, c} - 1 \right) + \lambda_2 \left(\sum_{c \in C} p_c - 1 \right)$$
            </p>
            <p>よって，これを偏微分して，0と等しいものとすると，
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial q_{w, c}} = \frac{\alpha - 1}{q_{w, c}} + \frac{n_{w, c}}{q_{w, c}} + \lambda_{1c} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial p_c} = \frac{\alpha - 1}{p_c} + \frac{N_c}{p_c} + \lambda_{2} = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_{1c}} = \sum_{w \in V} q_{w, c} - 1 = 0$$
                $$\frac{\partial L(\theta, \boldsymbol{\lambda_1}, \lambda_2)}{\partial \lambda_2} = \sum_{c \in C} p_c - 1 = 0$$
                となる。
            </p>
            <p>この連立方程式を解くと，
                $$q_{w, c} = \frac{n_{w, c} + \alpha - 1}{\dis{\sum_{w}} n_{w, c} + |W| (\alpha - 1)}$$
                $$p_c = \frac{N_c + \alpha - 1}{\dis{\sum_c} N_c + |C|(\alpha - 1)}$$
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>章末問題</h3>
        <div class="def " id="2">
            <span class="def-t ">
                <h4>【 2 】</h4>
            </span>
           <p>多項モデルのナイーブベイズ分類器を少し変形することを考えよう。クラスが$c$であるような文書$d$において，単語$w$が$k$回出現する確率は，ポアソン分布$P(k; \mu_{w, c}) = \cfrac{\mu_{m, c}^k e^{-\mu_{m, c}}}{k!}$に従うとする。このようなモデルのナイーブベイズ分類器を作りたい。$P(d | c)$を求めよ。また，最尤推定を用いてパラメータを推定する式を求めよ。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
            <p>$P(d|c)$は，
                $$P(d|c) = P\left(\sum_w n_{w, d}\right)\prod_w \frac{\mu_{w, c}^{n_{w, d}}}{n_{w, d}!} e^{-\mu_{w, c}}$$
            となる。</p>
            <p>対数尤度は，
                $$\begin{align*}
                    \log P(D) &= \sum_{d\in D} \log p_c P\left(\sum_w n_{w, d} \right) \prod_w \frac{\mu_{w, c}^{n_{w, d}}}{n_{w, d}!} e^{-\mu_{w, c}} \\
                    &= \sum_{d\in D} \log p_c P\left(\sum_w n_{w, d} \right) + \sum_c \sum_w n_{w, c}\log \mu_{w, c} - \sum_c N_c \sum_w \mu_{w, c} - \sum_c N_c \sum_w \log n_{w, c}!
                \end{align*}$$
                となる。
            </p>
            <p>これを$\mu_{w, c}$で偏微分して，
                $$\frac{\partial \log P(D)}{\partial \mu_{w, c}} = \frac{n_{w, c}}{\mu_{w, c}} - N_c$$
                となるから，これが0と等しいとき，最大値を取るので，
                $$\mu_{w, c} = \frac{n_{w, c}}{N_c}$$
            </p>

        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="3">
            <span class="def-t ">
                <h4>【 3 】</h4>
            </span>
            <p>SVMにおける最適化問題
                $$\min. \; \frac{1}{2} \bm{w}^2$$
                $$\mathrm{s.t. }\; y^{(i)} (\bm{w} \cdot \bm{x}^{(i)} - b) - 1 \geqq 0; \forall i$$
                は下に凸な凸計画問題であることを示せ。
            </p>
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>

        <div class="def " id="4">
            <span class="def-t ">
                <h4>【 4 】</h4>
            </span>
            <p>訓練データ$D = \{(\bm{x}^{(1)}, -1), (\bm{x}^{(2)}, 1), (\bm{x}^{(3)}, 1)\}$に対し，SVMを構築せよ。ただし，$\bm{x}^{(1)} = \left(0, \cfrac{1}{3} \right), \bm{x}^{(2)} = (1, 1), \bm{x}^{(3)} = (1, 0)$であるとする。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="rei">
            <span class="def-t ">
                <h4>例題4.11</h4>
            </span>
            <p></p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>

        <h3>章末問題</h3>
        <div class="def " id="5">
            <span class="def-t ">
                <h4>【 5 】</h4>
            </span>
            <p>緩和制約下のSVMについて，そのパラメータ$b$は，$0 \lt a_i \lt C$なる事例を一つ持って来て$b = \bm{w} \cdot \bm{x}^{(i)} - y^{(i)}$とすることで計算できることを示せ。KKT条件(A. 3)を用いればよい。</p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="def " id="6">
            <span class="def-t ">
                <h4>【 6 】</h4>
            </span>
            <p>ラベル付きデータ$D^l$とラベル無しデータ$D^u$が与えられたとする。
                $$\log P(D^l)P(D^u) = \sum_{(d, c) \in D^l} \log P(d, c) + \sum_{d \in D^u} \log P(d)$$
                がデータの対数尤度である。EMアルゴリズムを用いて，これらのデータから多項モデルのナイーブベイズ分類器を構築せよ。クラス変数が隠れ変数となる($Q$関数を作る際，隠れ変数に関する期待値は$D^u$内の事例についてのみとればよい)。
            </p>
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <h3>教科書</h3>
        <div class="def " id="4-5-2">
            <span class="def-t ">
                <h4>4.5.2 対数線形モデルの更新式の導出</h4>
            </span>
            
        </div>

        <div class="ans">
            <span class="ans-t">
                <h5>解答</h5>
            </span>
           
        </div>
        <p style="text-align:right;"><a href="#top" class="btn-square-slant ">↑トップに戻る</a></p>

        <div class="flex">
            <p style="width:33%;"><a href="./note3.html " class="btn-square-slant ">←前のページに移る</a></p>
            <p style="width:33%;"><a href="../index.html " class="btn-square-slant ">トップページに戻る</a></p>
            <p style="width:33%;"><a href="./note5.html " class="btn-square-slant ">次のページに移る→</a></p>
        </div>

        <br>

</body>

</html>